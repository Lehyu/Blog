## 特征工程
常用的的特征工程：数据预处理（标准化／归一化，离散化，缺失值处理。。。），特征选择，降维

![beta plot](https://github.com/Lehyu/lehyu.cn/blob/master/image/feature_engineering.jpg?raw=true)
### 数据不均衡怎么处理？
参考[csdn的博文](http://blog.csdn.net/sinat_26917383/article/details/75890859)，上采样训练出来的模型会有一定的过拟合；下采样只学到了整体样本的一部分，因此会欠拟合；其中对于上采样来说，在每次生成新数据点的时候加入轻微的随机扰动，会比较有效。

EasyEnsemble：多次有放回下采样，产生多个不同的训练集，训练多个模型

BalanceCascade：先通过一次下采样产生训练集，训练一个分类器，对于分类正确的样本不放回，分类错误的样本放回，然后再下采样，训练第二个模型，以此类推。

基于聚类的重复抽样方法：首先分别对正负样本进行K-means聚类，聚类后进行Oversampling等系列方法。不仅能够解决类间不平衡问题，而且还能解决类内部不平衡问题。

如果计算资源足够且小众类样本足够多的情况下使用上采样，否则使用下采样，因为上采样会增加训练集的大小进而增加训练时间，同时小的训练集非常容易产生过拟合。对于下采样，如果计算资源相对较多且有良好的并行环境，应该选择Ensemble方法。

### 标准化、归一化、正则化
标准化：(x-mean)/std，计算时分别对每个属性进行计算

归一化将数据映射到（0，1）区间，把有量纲表达式变成无量纲表达式，常用的有线性转换（(x-min)/(max-min)），对数转换（log10(x)）等

正则化：将每个样本缩放到单位向量上

标准化／归一化是依照特征矩阵的列处理数据，将样本的特征值转换到同一量纲下。正则化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。

![beta plot](https://github.com/Lehyu/lehyu.cn/blob/master/image/standard_normalizer.jpg?raw=true)

如果采用标准化，不改变样本在这两个维度上的分布，则左图还是会保持二维分布的一个扁平性；而采用正则化则会在不同维度上对数据进行不同的伸缩变化（归一区间，会改变数据的原始距离，分布，信息），使得其呈类圆形。虽然这样样本会失去原始的信息，但这防止了正则化前直接对原始数据进行梯度下降类似的优化算法时最终解被数值大的特征所主导。正则化之后，各个特征对目标函数的影响权重是一致的。这样的好处是在提高迭代求解的精度。

### 离散化

离散化后，得到稀疏向量，内积乘法运算速度快，计算结果方便存储；对异常数据具有鲁棒性；离散化后再进行OneHot编码，能够为模型引入非线性，提升模型的表达能力；同时，离散化后可以进行特征交叉，进一步提高模型表达能力；特征离散化后，模型会更稳定

### 降维
参考[csdn博客](http://blog.csdn.net/dongyanwen6036/article/details/78311071)

LDA与PCA降维：1）出发思想不同，PCA主要从特征的协方差角度出发，选择样本点投影具有最大方差的方向；而LDA考虑分类标签的信息，寻求投影后不同类别之间的数据点距离最大化以及同一类别数据距离最小化；2）学习模式不同，PCA为无监督学习，LDA为监督学习；

### 特征选择

#### Filter（过滤式）
自变量与目标变量之间的关联，与特定的算法无关，通用性强，可以去除大量不想管的特征，可作为预筛选器，比如选择方差较大的特征
#### Wrapper（包裹式）
利用学习期算法的性能来评价特征子集的优劣，性能好，通用性弱，当改变学习算法时，需要针对该算法重新进行特征选择
#### Embedded（嵌入式）
把特征选择作为组成部分嵌入到学习算法中，如决策树、Lasso、深度学习等

#### 卡方检验


### RandomForest的feature_importance与GBDT／XGBoost不一致
这是由于他们是不同的框架下的继承算法，RF为Bagging框架，GBDT／XGBoost为boosting框架，在Bagging框架中，每个基学习器的特征集独立抽样出来的，因此相关性较强的特征可能会出现在多个基学习器中；而Boosting框架会趋向于只使用相关特征中的一个，其他相关特征使用较少，因为相关特征不会为模型带来更多的信息。


## LR
## SVM
## DecisionTree
## RandomForest
随机森林的特征重要性，假设特征 $X_j$ 在节点m的重要性为 VIM_jm = Gini_m-Gini_l-Gini_r，那么在这课决策树上的重要性为 $\sum_{m \in M} VIM_jm$，整个森林中的重要性为 $VIM_j = \sum_{i=1}^n VIM_{ij}$

## GBDT
## XGBoost
## 模型比较
