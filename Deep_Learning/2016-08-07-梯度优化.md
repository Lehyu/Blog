---
layout:  post
title:  "梯度优化"
date:	2016-08-07 16:30:43 +0800
categories: [Deep Learning]
---
## 神经网络的表示
假设一个深度神经网络有$nl$层网络，输入为 $(X,y)$ 那么深度神经网络可以一般表示为
 $$y^{'}=f(W,X)=f_{nl}(W^{nl},f_{nl-1}(W^{nl-1},f_{nl-2}(...f_1(W^1,X)...)))$$
即前一层的输出是下一层的输入， 也就是任意一层可以看作是关于前一层输出的函数。
训练深度神经网络，也就是使 $$y$$ 与 $$y^{'}$$ 之间的误差最小，即关于 $$y$$ 和 $$y^{'}$$ 的函数 $$L(y,y^{'})$$ 最小。由 $$y^{'}$$ 的表达式我们可得
$$L(y,y^{'})=L(y,f(W,X))$$
由于 $$(X,y)$$ 已知，故求解 $$L(y,y^{'})$$ 的最小值问题又可以转化成如何调整 $$W$$ 的值从而使 $$L$$ 最小。
需要注意的是在前馈（forward）中，即求 $$y^{'}$$ 时 $$y^{'}$$ 是关于 $$X$$ 的函数，而在BP中，是调整权值 $$W$$ 以使损失函数 $$L$$ 达到最小。

## 梯度下降
对于一个函数 $$y=f(w,x)$$ , 我们知道，对于任意 $$\epsilon > 0$$ , 并且当 $$\epsilon$$ 足够小时，$$f(x+\epsilon)\approx f(x)+\epsilon f^{'}(x)$$ 。 因此导数在求损失函数的最小值时，非常有用。当 $$\epsilon$$ 足够小时，$$f(x-\epsilon f^{'}(x)) \approx f(x) - \epsilon (f^{'}(x))^{2} < f(x)$$ 。
故梯度下降可以表示为 $$x = x - \epsilon f^{'}(x)$$ 。
![alt image](https://raw.githubusercontent.com/lehyu/lehyu.github.com/master/image/DL/Gradient_Descent/gradient_descent.png)
理论上，当 $$x$$ 达到最值点时，由于 $$f^{'}(x)=0$$ ，因此梯度不会再变化，此时的 $$x$$ 即为我们所求的解。 但是在实际应用中，往往不会达到最优点，一个原因损失函数通常为非凸函数，从而导致陷入局部最优点；另一个原因是 $$\epsilon$$ 的值不可能选的足够小（训练效率，$$\epsilon$$ 太小导致训练速度慢，太大导致训练不能收敛），从而导致最后在最值点附近震荡。

### 伪代码
1. 初始化：随机初始化一个点
2. 计算当前损失函数的梯度：  $$V=\nabla f(X) = <\frac{df}{dx_1}, \frac{df}{dx_2}, \dots, \frac{df}{dx_n}>$$
3. 更新：$$X_{i+1} = X_i - \epsilon V$$
4. 重复步骤2-3，知道满足迭代条件

## 牛顿法
理论上，梯度下降能够很好的满足我们的要求，但是在实际应用中，梯度下降法训练速度太慢，因此只有在数据较小时采用梯度下降法，数据过大时，一般采用其他方法。

### 泰勒公式
对于正整数 $$n$$ ，若函数 $f(x)$ 在闭区间 $$[a,b]$$ 上 $$n$$ 阶连续可导， 且在 $$(a,b)$$ 上 $$n+1$$ 阶可导。 $$\quad\forall x \in [a,b]$$ 满足一下公式：
$$
f(x)=f(a)+f^{'}(a)(x-a)+\frac{f^{''}(a)}{2!}(x-a)^2+\dots+\frac{f^n(a)}{n!}(x-a)^n+R_n(x)
$$
具体参考[百度百科](http://baike.baidu.com/link?url=_kdsstV9avqZ5VpambHuddyq3MOHNT2MVYOpx2UoSf24HYgkWPnomBP2R3Ho0hYfkamqmFL4s922Xm5anzkHj_#1) 。

### 牛顿法的表述
则当 $$\epsilon$$ 足够小时，我们得 $$f(x_0+\epsilon)\approx f(x_0)+f^{'}(x_0)\epsilon +f^{''}(x_0) \frac{\epsilon^2}{2}$$ ， 我们知道最值点处 $$\frac{df}{dx} = 0$$， 即我们想求这样的一个 $$\epsilon$$ 使得 $$x_0+\epsilon$$ 为最优点，从而有 $$\frac{df}{dx} = f^{'}(x_0)+f^{''}(x_0)\epsilon = 0$$ ，求得 $$\epsilon = -\frac{f^{'}(x_0)}{f^{''}(x_0)}$$， 从而有
$$x_{i+1} = x_i-{f^{''}(x_i)}^{-1}{f^{'}(x_i)}$$
当 $$f:\mathbb R^n \to R$$ 时，$$f^{''}(x)\to H(f)(x)$$ ， 其中 $$H(f)$$ 为 [Hessian 矩阵](http://baike.baidu.com/link?url=z7ePAFFkiNpdqNITDJej4gsBDYEiVbWzUmz5PRGXS_fgQnmHmsdOwWTzHZK0SwEmoxy7aR3SQMWa2EJteFmNBq)。

### 牛顿法的优缺点
为什么理论上牛顿法会比梯度下降法要更高效？由梯度下降法和牛顿法的更新方式我们可以知道，他们的区别在于，梯度下降法的更新是固定步长（如果我们忽略一阶导数的话），然后按照附体读的方向一步一步的走；而很明显牛顿法的更新是观念时直接求梯度等于0的点，目的是为了一步跨到最优点，只不过是因为求解时近似的，导致这一步走的不太准，还需要迭代。
![alt Newton_optimization_vs_grad_descent](https://raw.githubusercontent.com/lehyu/lehyu.github.com/master/image/DL/Gradient_Descent/Newton_optimization_vs_grad_descent.png)
上图中的红线表示牛顿法的优化过程，而绿线表示梯度下降法的优化过程。
但是实际上，牛顿法效率可能更差，因为计算Hessian矩阵存在两个难点，一是在神经网络中计算Hessian矩阵需要额外的算法，BP算法能够计算梯度; 二是计算Hessian矩阵需要较大的内存，假如数据是 $$n$$ 维的话，那么需要 $$O(n^2)$$ 的内存空间。

## 共轭梯度法
共轭梯度法是用于优化二次函数的方法，但是我们可以通过结合牛顿法来优化其他非线性函数。首先我们来了解一下共轭梯度法。梯度下降法在偏离最优点的位置优化速度比较快，但是在最优点附近优化速度会变得非常慢，这是因为学习率 $$\epsilon$$ 是固定的，而在最优点附近的梯度十分小，从而导致速率慢。那么如何解决这个问题呢？是否能使 $$\epsilon$$ 随着优化过程更更改它的值从而每次更新的速率都达到最优？

### 理论
对于任意二次函数，我们可以写成以下形式，
$$f(x) = \frac{1}{2}x^T A x + b^T x + c$$
其中 $$A=A^T$$ 。那么其梯度为 $$\nabla f(x) = A x + b$$ ，令 $$g(\epsilon) = f(x_0-\epsilon\nabla f(x_0))$$，则 $$g'(\epsilon) = ({\nabla f(x_0)}^T A {\nabla f(x_0)})\epsilon + {\nabla f(x_0)}^T(A x_0 + b) = 0$$ ，即 $$\epsilon = -\frac{(\nabla f(x_i))^T (A x_i + b)}{(\nabla f(x_i))^T A \nabla f(x_i)}$$ 。

从而 $$x_1 = x_0 - \epsilon \nabla f(x_0)$$

### 伪代码
1. 初始化：令 $$i=0$$ 和 $$x_i=x_0$$，计算 $$d_i=d_0=-\nabla f(x_0)$$
2. 找到最好的学习率 $$\epsilon$$ : $$\epsilon = -\frac{ {d_i}^T (A x_i + b)}{(d_i)^T A d_i}$$
3. 更新权值： $$x_{i+1} = x_i + \epsilon d_i$$
4. 更新方向： $$d_{i+1} = -\nabla f(x_{i+1}) + \beta_i d_i$$， 其中
$$\beta_i = \frac{\nabla f(x_{i+1})^T A d_i}{ {d_i}^T A d_i}$$
5. 重复步骤2-4直到满足迭代条件。

### Hessian-free优化
那么对于其他非二次函数，该怎么优化呢？由牛顿法我们知道 $$f(x + \Delta x) \approx f(x) + \nabla f(x)^T \Delta x + {\Delta x}^T H(f) \Delta x$$ 。因此可以将其他函数转化成二次函数来优化。
1. 初始化
2. 转化成二次函数：在当前点 $$x_i$$ 计算 $$\nabla f(x_i)$$ 和 $$H(f)(x_i)$$，将其转化为 $$f(x_i + \Delta x) \approx f(x_i) + \nabla f(x_i)^T \Delta x + {\Delta x}^T H(f) \Delta x$$
3. 共轭梯度：利用共轭梯度法计算 $$x_{i+1}$$ 。
4. 重复步骤2-3

## 参考资料
Martens, J. (2010). Deep learning via Hessian-free optimization. Proceedings of the 27th International Conference on Machine Learning (ICML-10).
